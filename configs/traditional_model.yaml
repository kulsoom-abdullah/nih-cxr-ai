# configs/test_config.yaml
data:
  class_path: nih_cxr_ai.data.nih_dataset.NIHChestDataModule
  init_args:
    data_dir: data/nih_chest_xray
    batch_size: 32
    num_workers: 4
    image_size: [224, 224]
    train_val_test_split: [0.7, 0.15, 0.15]
    # Add these for testing
    subset_size: 1000  # Only use 1000 images total
    debug: true       # Enable debug mode

trainer:
  max_epochs: 1
  limit_train_batches: 0.1  # Use only 10% of training data
  limit_val_batches: 0.1    # Use only 10% of validation data
  limit_test_batches: 0.1   # Use only 10% of test data
  # ... rest of your trainer config
  

# configs/traditional_model.yaml
model:
  class_path: nih_cxr_ai.models.traditional.TraditionalCXRModel
  init_args:
    num_classes: 14  # Changed this to 14 to match the paper (only pathologies)
    learning_rate: 0.0001  # They used 1e-4 in the paper

data:
  class_path: nih_cxr_ai.data.nih_dataset.NIHChestDataModule
  init_args:
    data_dir: data/nih_chest_xray
    batch_size: 32
    num_workers: 4
    image_size: [224, 224]
    train_val_test_split: [0.7, 0.15, 0.15]

trainer:
  max_epochs: 50  
  accelerator: auto
  devices: auto
  precision: 16
  callbacks:
    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        monitor: val_auroc
        mode: max
        filename: "{epoch}-{val_auroc:.3f}"
        save_top_k: 3
    - class_path: pytorch_lightning.callbacks.EarlyStopping
      init_args:
        monitor: val_auroc
        patience: 10
        mode: max
    - class_path: pytorch_lightning.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step
  logger:
    class_path: pytorch_lightning.loggers.WandbLogger
    init_args:
      project: chest-xray-comparison
      log_model: true
